name: Build and Release

on:
  push:
    tags:
      - "v*"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    name: Build on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        include:
          - os: ubuntu-latest
            platform: linux
            arch: amd64
          - os: macos-15-intel # Intel Mac
            platform: darwin
            arch: amd64
          - os: macos-latest # Apple Silicon Mac
            platform: darwin
            arch: arm64
          - os: windows-latest
            platform: windows
            arch: amd64

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: "stable"

      - name: Set up build environment (Ubuntu)
        if: matrix.os == 'ubuntu-latest'
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake

      - name: Set up build environment (macOS)
        if: startsWith(matrix.os, 'macos')
        run: |
          brew install cmake

      - name: Install MinGW-w64 (Windows - MSVCRT)
        if: matrix.os == 'windows-latest'
        run: |
          # Download WinLibs MSVCRT version (compatible with Kali Linux cross-compilation)
          curl -L -o mingw.zip https://github.com/brechtsanders/winlibs_mingw/releases/download/14.2.0posix-19.1.1-12.0.0-msvcrt-r2/winlibs-x86_64-posix-seh-gcc-14.2.0-mingw-w64msvcrt-12.0.0-r2.zip
          7z x mingw.zip -oC:\mingw-msvcrt
          echo "C:\mingw-msvcrt\mingw64\bin" >> $env:GITHUB_PATH
        shell: pwsh

      - name: Install CMake (Windows)
        if: matrix.os == 'windows-latest'
        run: choco install cmake -y
        shell: pwsh

      - name: Get llama.cpp commit
        id: get_commit
        shell: bash
        run: |
          LLAMA_COMMIT=$(git submodule status core/llama.cpp | awk '{print $1}' | sed 's/^-//')
          echo "llama_commit=${LLAMA_COMMIT}" >> $GITHUB_OUTPUT
          echo "Llama.cpp commit: ${LLAMA_COMMIT}"

      - name: Build (Unix)
        if: runner.os != 'Windows'
        shell: bash
        run: |
          make

      - name: Build C++ library (Windows)
        if: runner.os == 'Windows'
        shell: bash
        run: |
          # Force apply patch before building
          echo "Applying memory loading patch..."
          cd core/llama.cpp
          # Remove marker file to force reapply
          rm -f .patch_applied
          # Try to apply patch, ignore errors if already applied
          git apply ../patches/memory-loading.patch 2>&1 || true
          # Always create marker file
          touch .patch_applied
          cd ../..

          export CC=gcc
          export CXX=g++
          bash ./scripts/build_cpp.sh

      - name: Build Go binaries (Windows)
        if: runner.os == 'Windows'
        shell: bash
        run: |
          export CC=gcc
          export CXX=g++
          echo "Using CC=$CC"
          echo "Using CXX=$CXX"

          # Build llama binary
          GITVER=$(git rev-parse --short=7 HEAD)
          GITDIRTY=$(git diff --quiet || echo '-dirty')
          GITVERSION="${GITVER}${GITDIRTY}"
          versionBuild="github.com/Qitmeer/llama.go/version.Build=dev-${GITVERSION}"

          cd ./cmd/llama
          go build -ldflags "-X ${versionBuild}" -o ../../build/bin/llama.exe

          # Build modelembed binary
          cd ../modelembed
          go build -o ../../build/bin/modelembed.exe

      - name: Verify build output
        shell: bash
        run: |
          echo "Checking build output:"
          ls -la build/bin/
          if [ "${{ matrix.os }}" == "windows-latest" ]; then
            ./build/bin/llama.exe --version || echo "Binary execution failed"
          else
            ./build/bin/llama --version || echo "Binary execution failed"
          fi

      - name: Create release package
        shell: bash
        run: |
          RELEASE_DIR="llama.go-${{ matrix.platform }}-${{ matrix.arch }}"
          TEMP_DIR=$(mktemp -d)

          # Copy entire repository to temp directory
          cp -r . "$TEMP_DIR/repo"

          # Move to the release directory name
          mv "$TEMP_DIR/repo" "$RELEASE_DIR"

          # Save llama.cpp commit hash
          echo "${{ steps.get_commit.outputs.llama_commit }}" > "$RELEASE_DIR/LLAMA_COMMIT"

          # Create usage instructions
          cat > "$RELEASE_DIR/USAGE.md" << 'EOF'
          # Using llama.go as a Library

          ## Quick Start

          1. Extract this package
          2. Import in your Go project:

          ```go
          import "github.com/Qitmeer/llama.go/wrapper"

          // Load model from memory
          modelData := []byte{...} // your model data
          wrapper.LoadFromMemory(modelData, "llama --ctx-size 2048")

          // Create channel and run inference
          id, ch := wrapper.NewChan()
          defer wrapper.CloseChan(id)

          go wrapper.LlamaGenerate(id, `{"prompt":"Hello","stream":true}`)

          // Receive tokens
          for token := range ch {
              fmt.Print(token)
          }
          ```

          ## Building from Source

          If you need to rebuild:

          ```bash
          make clean
          make
          ```

          ## Pre-built Components

          This package includes:
          - Pre-built llama.cpp libraries in `build/`
          - All Go source code
          - Build scripts and Makefile
          - Core C/C++ source and headers

          ## Platform

          - OS: ${{ matrix.platform }}
          - Architecture: ${{ matrix.arch }}
          - llama.cpp commit: ${{ steps.get_commit.outputs.llama_commit }}
          EOF

          # Verify package contents
          echo "Package contents:"
          ls -la "$RELEASE_DIR/"
          echo ""
          echo "Build artifacts:"
          ls -la "$RELEASE_DIR/build/" || echo "No build directory"
          echo ""
          echo "Go packages:"
          find "$RELEASE_DIR" -name "*.go" -type f | wc -l
          echo "files"

      - name: Create tarball (Unix)
        if: runner.os != 'Windows'
        run: |
          RELEASE_DIR="llama.go-${{ matrix.platform }}-${{ matrix.arch }}"
          tar -czf "${RELEASE_DIR}.tar.gz" "$RELEASE_DIR"

      - name: Create zip (Windows)
        if: runner.os == 'Windows'
        run: |
          Compress-Archive -Path "llama.go-${{ matrix.platform }}-${{ matrix.arch }}" -DestinationPath "llama.go-${{ matrix.platform }}-${{ matrix.arch }}.zip"

      - name: Upload artifact (Unix)
        if: runner.os != 'Windows'
        uses: actions/upload-artifact@v4
        with:
          name: llama.go-${{ matrix.platform }}-${{ matrix.arch }}
          path: llama.go-${{ matrix.platform }}-${{ matrix.arch }}.tar.gz

      - name: Upload artifact (Windows)
        if: runner.os == 'Windows'
        uses: actions/upload-artifact@v4
        with:
          name: llama.go-${{ matrix.platform }}-${{ matrix.arch }}
          path: llama.go-${{ matrix.platform }}-${{ matrix.arch }}.zip

  release:
    needs: build
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/') || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Get llama.cpp commit
        id: get_commit
        run: |
          LLAMA_COMMIT=$(git submodule status core/llama.cpp | awk '{print $1}' | sed 's/^-//')
          echo "llama_commit=${LLAMA_COMMIT}" >> $GITHUB_OUTPUT
          echo "Llama.cpp commit: ${LLAMA_COMMIT}"

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Debug artifacts
        run: |
          echo "Artifacts directory:"
          ls -la artifacts/
          find artifacts -type f

      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          tag_name: ${{ github.ref_name || format('manual-{0}', github.run_number) }}
          name: Release ${{ github.ref_name || format('manual-{0}', github.run_number) }}
          body: |
            llama.go with memory loading feature - Ready to use as a library or standalone binary

            **llama.cpp commit**: `${{ steps.get_commit.outputs.llama_commit }}`

            ## Features
            - Memory-based model loading without creating temporary files
            - Support for mmap on all platforms (Linux, macOS, Windows)
            - macOS automatic fallback to memory loading for large models
            - Self-contained executable support with embedded models
            - Ready to use as a Go library

            ## What's Included

            Each platform package contains:
            - Pre-built llama.cpp libraries (`build/`)
            - All Go source code (wrapper, api, config, etc.)
            - Build scripts and Makefile
            - C/C++ headers and source files
            - go.mod and go.sum for dependency management
            - USAGE.md with library integration examples

            ## Using as a Go Library

            Extract the package and import in your project:

            ```go
            import "github.com/Qitmeer/llama.go/wrapper"

            // Load model from memory
            modelData := []byte{...} // your model data or go:embed
            wrapper.LoadFromMemory(modelData, "llama --ctx-size 2048")

            // Run inference
            id, ch := wrapper.NewChan()
            defer wrapper.CloseChan(id)

            go wrapper.LlamaGenerate(id, `{"prompt":"Hello","stream":true}`)

            for token := range ch {
                fmt.Print(token)
            }
            ```

            See `USAGE.md` in the package for more examples.

            ## Using as Standalone Binary

            ### Linux (amd64)
            ```bash
            tar xzf llama.go-linux-amd64.tar.gz
            cd llama.go-linux-amd64
            ./build/bin/llama --version
            ```

            ### macOS (Intel)
            ```bash
            tar xzf llama.go-darwin-amd64.tar.gz
            cd llama.go-darwin-amd64
            ./build/bin/llama --version
            ```

            ### macOS (Apple Silicon)
            ```bash
            tar xzf llama.go-darwin-arm64.tar.gz
            cd llama.go-darwin-arm64
            ./build/bin/llama --version
            ```

            ### Windows (amd64)
            Extract `llama.go-windows-amd64.zip` and run:
            ```powershell
            .\build\bin\llama.exe --version
            ```

            ## Building from Source

            If you need to rebuild:
            ```bash
            cd llama.go-<platform>-<arch>
            make clean
            make
            ```
          draft: ${{ github.event_name == 'workflow_dispatch' }}
          prerelease: ${{ github.event_name == 'workflow_dispatch' }}
          files: |
            artifacts/llama.go-*/*
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
